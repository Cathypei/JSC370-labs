---
title: "Lab 10 - Trees, Bagging, RF, Boosting XGBoost"
author: "Cathy Pei"
date: "Mar 15, 2024"
output: html_document
---

```{r setup, echo=FALSE, eval=TRUE}
knitr::opts_chunk$set(eval = F, include  = T, echo = T)
```

# Learning goals

- Perform classification and regression with tree-based methods in R
- Recognize that tree-based methods are capable of capturing non-linearities by splitting multiple times on the same variables
- Compare the performance of classification trees, bagging, random forests, and boosting for predicting heart disease based on the ``heart`` data.

# Lab description

For this lab we will be working with the `heart` dataset that you can download from [here](https://github.com/JSC370/JSC370-2024/blob/main/data/heart.csv)


### Setup packages

You should install and load `rpart` (trees), `randomForest` (random forest), `gbm` (gradient boosting) and `xgboost` (extreme gradient boosting).


```{r}
install.packages(c("rpart", "rpart.plot", "randomForest", "gbm", "xgboost"))
```

### Load packages and data
```{r warning=FALSE, message=FALSE, eval=TRUE}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
library(caret)

heart <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2024/main/data/heart.csv")%>% mutate(
    AHD = 1 * (AHD == "Yes"),
    ChestPain = factor(ChestPain),
    Thal = factor(Thal)
  )

head(heart)
```


---


## Question 1: Trees
- Split the `heart` data into training and testing (70-30%)

```{r eval=TRUE}

train<-sample(1:nrow(heart), round(0.7*nrow(heart)))
heart_train<-heart[train,]
heart_test<-heart[-train,]
```

- Fit a classification tree using rpart, plot the full tree. We are trying to predict AHD. Set minsplit = 10, minbucket = 3, and do 10 cross validations.
```{r eval=TRUE}
heart_tree <- rpart(AHD ~ ., data = heart_train, method = "class", 
                    minsplit = 10, minbucket = 3, control = rpart.control(cp = 0.01), 
                    parms = list(split = "information"), 
                    xval = 10)
rpart.plot(heart_tree)
```

- Plot the complexity parameter table for an rpart fit and find the optimal cp
```{r eval=TRUE}
plotcp(heart_tree)
printcp(heart_tree)

optimalcp <- heart_tree$cptable[which.min(heart_tree$cptable[, "xerror"]), "CP"]
```

- Prune the tree

```{r eval=TRUE}
heart_tree_prune <- prune(heart_tree, cp = optimalcp)
rpart.plot(heart_tree_prune)
```

- Compute the test misclassification error

```{r eval=TRUE}
heart_pred <- predict(heart_tree_prune, heart_test, type = "class")
misclassification_error <- mean(heart_pred != heart_test$AHD)
print(misclassification_error)
```

- Fit the tree with the optimal complexity parameter to the full data (training + testing)

```{r eval=TRUE}
heart_tree <- rpart(AHD ~ ., data = heart, method = "class", 
                    minsplit = 10, minbucket = 3, control = rpart.control(cp = optimalcp), 
                    parms = list(split = "information"))
rpart.plot(heart_tree)
```

- Find the Out of Bag (OOB) error for tree

```{r warning=FALSE, eval=TRUE}
heart_clean <- na.omit(heart)

rf_model <- randomForest(AHD ~ ., data = heart_clean, ntree = 100, proximity = TRUE)

oob_error <- rf_model$mse[1]
print(oob_error)

```


---

## Question 2: Bagging, Random Forest

- Compare the performance of classification trees (above), bagging, random forests for predicting heart disease based on the ``heart`` data.

- Use the training and testing sets from above. Train each of the models on the training data and extract the cross-validation (or out-of-bag error for bagging and Random forest). 


- For bagging use ``randomForest`` with ``mtry`` equal to the number of features (all other parameters at their default values). Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r eval=TRUE}
heart_train <- na.omit(heart_train)
heart_test <- na.omit(heart_test)
heart_train$AHD <- as.factor(heart_train$AHD)
heart_test$AHD <- as.factor(heart_test$AHD)

heart_bag <- randomForest(AHD ~ ., data = heart_train, ntree = 500, mtry = ncol(heart_train)-1)
# oob error rate
bag_oob_error <- mean(heart_bag$err.rate[,1])
print(paste("Bagging OOB error rate:", bag_oob_error))

varImpPlot(heart_bag)
bag_variable_importance <- importance(heart_bag)
print(bag_variable_importance)
```

- For random forests use ``randomForest`` with the default parameters. Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r eval=TRUE}
heart_rf <- randomForest(AHD ~ ., data = heart_train, ntree = 500)
#oob error rate
rf_oob_error <- mean(heart_rf$err.rate[,1])
print(paste("Random Forest OOB error rate:", rf_oob_error))

varImpPlot(heart_rf)
rf_variable_importance <- importance(heart_rf)
print(rf_variable_importance)
```
---

# Question 3: Boosting

- For boosting use `gbm` with ``cv.folds=5`` to perform 5-fold cross-validation, and set ``class.stratify.cv`` to ``AHD`` (heart disease outcome) so that cross-validation is performed stratifying by ``AHD``.  Plot the cross-validation error as a function of the boosting iteration/trees (the `$cv.error` component of the object returned by ``gbm``) and determine whether additional boosting iterations are warranted. If so, run additional iterations with  ``gbm.more`` (use the R help to check its syntax). Choose the optimal number of iterations. Use the ``summary.gbm`` function to generate the variable importance plot and extract variable importance/influence (``summary.gbm`` does both). Generate 1D and 2D marginal plots with ``gbm.plot`` to assess the effect of the top three variables and their 2-way interactions. 

```{r eval=TRUE}
# Train the boosting model with cross-validation
heart_boost <- gbm(AHD ~ ., data = heart, 
                   distribution = "bernoulli", 
                   cv.folds = 5, 
                   n.trees = 1000, 
                   verbose = TRUE)

# Plot the cross-validation error as a function of boosting iteration
plot(heart_boost$cv.error, type = "l", xlab = "Boosting Iteration", ylab = "Cross-validation Error")

# Determine whether additional boosting iterations are warranted
# the CV error is not decreasing so no need to run additional iterations

# Use summary.gbm to generate variable importance plot and extract variable importance/influence
print(summary(heart_boost))
```
```{r eval=TRUE}
# Get the top three variables
top_three_vars <- c("Thal", "ChestPain", "Ca")

# Generate marginal plots for the top three variables
for (var in top_three_vars) {
  print(plot.gbm(heart_boost, i.var = var, main = paste("Marginal Plot for", var)))
}

```
```{r eval=TRUE}
# Generate two-way interaction plots for the top three variables
Thal_Ca <- c("Thal", "Ca")
Thal_ChestPain <- c("Thal", "ChestPain")
Ca_ChestPain <- c("ChestPain", "Ca")
print(plot.gbm(heart_boost, i.var = Thal_Ca, 
         i.var2 = Thal_Ca, n.plots = 2,
         main = "2-Way Interactions of Thal and Ca"))
print(plot.gbm(heart_boost, i.var = Thal_ChestPain, 
         i.var2 = Thal_ChestPain, n.plots = 2,
         main = "2-Way Interactions of Thal and ChestPain"))
print(plot.gbm(heart_boost, i.var = Ca_ChestPain, 
         i.var2 = Ca_ChestPain, n.plots = 2,
         main = "2-Way Interactions of Ca and ChestPain"))
```


---


## Question 4: Gradient Boosting

Evaluate the effect of critical boosting parameters (number of boosting iterations, shrinkage/learning rate, and tree depth/interaction).  In ``gbm`` the number of iterations is controlled by ``n.trees`` (default is 100), the shrinkage/learning rate is controlled by ``shrinkage`` (default is 0.001), and interaction depth by ``interaction.depth`` (default is 1).

Note, boosting can overfit if the number of trees is too large. The shrinkage parameter controls the rate at which the boosting learns. Very small $\lambda$ can require using a very large number of trees to achieve good performance. Finally, interaction depth controls the interaction order of the boosted model. A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc. the default is 1.


- Set the seed and train a boosting classification with ``gbm`` using 10-fold cross-validation (``cv.folds=10``) on the training data with ``n.trees = 5000``, ``shrinkage = 0.001``, and ``interaction.depth =1``. Plot the cross-validation errors as a function of the boosting iteration and calculate the test MSE.

```{r eval=TRUE}
set.seed(301)
train <- sample(1:nrow(heart), round(0.7 * nrow(heart)))
test <- setdiff(1:nrow(heart), train)
# First combination of parameters
heart_boost_1 <- gbm(AHD ~ ., data = heart[train, ], 
                     distribution = "bernoulli", 
                     n.trees = 5000, 
                     interaction.depth = 1, 
                     shrinkage = 0.001, 
                     cv.folds = 10, 
                     class.stratify.cv = TRUE)

summary_boost_1 <- summary(heart_boost_1)

yhat_boost_1 <- predict(heart_boost_1, newdata = heart[test, ])
mse_1 <- mean(unlist((heart[test, "AHD"] - yhat_boost_1)^2))
print(mse_1)
```

- Repeat the above using the same seed and ``n.trees=5000`` with the following 3 additional combination of parameters: a) ``shrinkage = 0.001``, ``interaction.depth = 2``; b) ``shrinkage = 0.01``, ``interaction.depth = 1``; c) ``shrinkage = 0.01``, ``interaction.depth = 2``.

```{r eval=TRUE}
set.seed(301)
train <- sample(1:nrow(heart), round(0.7 * nrow(heart)))
test <- setdiff(1:nrow(heart), train)

# Second combination of parameters
heart_boost_2 <- gbm(AHD ~ ., data = heart[train, ], 
                     distribution = "bernoulli", 
                     n.trees = 5000, 
                     interaction.depth = 2, 
                     shrinkage = 0.001, 
                     cv.folds = 10, 
                     class.stratify.cv = TRUE)

summary_boost_2 <- summary(heart_boost_2)

yhat_boost_2 <- predict(heart_boost_2, newdata = heart[test, ])
mse_2 <- mean(unlist((heart[test, "AHD"] - yhat_boost_2)^2))
print(mse_2)

# Third combination of parameters
heart_boost_3 <- gbm(AHD ~ ., data = heart[train, ], 
                     distribution = "bernoulli", 
                     n.trees = 5000, 
                     interaction.depth = 1, 
                     shrinkage = 0.01, 
                     cv.folds = 10, 
                     class.stratify.cv = TRUE)

summary_boost_3 <- summary(heart_boost_3)

yhat_boost_3 <- predict(heart_boost_3, newdata = heart[test, ])
mse_3 <- mean(unlist((heart[test, "AHD"] - yhat_boost_3)^2))
print(mse_3)

# Fourth combination of parameters
heart_boost_4 <- gbm(AHD ~ ., data = heart[train, ], 
                     distribution = "bernoulli", 
                     n.trees = 5000, 
                     interaction.depth = 2, 
                     shrinkage = 0.01, 
                     cv.folds = 10, 
                     class.stratify.cv = TRUE)

summary_boost_4 <- summary(heart_boost_4)

yhat_boost_4 <- predict(heart_boost_4, newdata = heart[test, ])
mse_4 <- mean(unlist((heart[test, "AHD"] - yhat_boost_4)^2))
print(mse_4)
```


## Question 5: Extreme Gradient Boosting

Training an xgboost model with `xgboost` and perform a grid search for tuning the number of trees and the maxium depth of the tree. Also perform 10-fold cross-validation and determine the variable importance. Finally, compute the test MSE.

```{r include=FALSE, eval=TRUE}

train_control = trainControl(method = "cv", number = 10, search ="grid")

tune_grid<-  expand.grid(max_depth = c(1, 3, 5, 7), #larger makes model more complex and potentially overfit
                        nrounds = (1:10)*50, # number of trees
                        # default values below
                        eta = c(0.01,0.1,0.3), # learning rate (equivalent to shrinkage)
                        gamma = 0, #Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.
                        subsample = 1,
                        min_child_weight = 1,
                        colsample_bytree = 0.6 #specify the fraction of columns to be subsampled
                        )

heart_imputed <- na.omit(heart)

heart_xgb <- caret::train(AHD ~ ., 
                          data = na.omit(heart_imputed[train, ]), 
                          method = "xgbTree",
                          trControl = train_control,
                          tuneGrid = tune_grid
)

```
```{r warning=FALSE, eval=TRUE}
varimp <- varImp(heart_xgb)
plot(varimp)

predictions <- predict(heart_xgb, newdata = heart[-train, ])

# Compute test MSE
test_mse <- mean((heart$AHD[-train] - predictions)^2)
print(test_mse)
```


- Compare the the performance of the different models and summarize

By comparing the test MSE values and OOB error rates across the models, we can see that the random forest model has a lower OOB error rate compared to the bagging model and classification tree model, indicating that it has overall better performance. Meanwhile, the extreme gradient boosting model has significantly lower test MSE values compared to the boosting and gradient boosting models, implying that it has better performance.

# Deliverables

1. Questions 1-5 answered, pdf or html output uploaded to quercus
